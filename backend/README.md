# Nyaya.exe Backend API

A production-style RAG (Retrieval-Augmented Generation) API for querying Nepali legal documents.

## ğŸ“ Project Structure

```
backend/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py              # FastAPI entry point
â”‚   â”œâ”€â”€ routes/              # API endpoints
â”‚   â”‚   â”œâ”€â”€ upload.py        # Document upload endpoint
â”‚   â”‚   â”œâ”€â”€ ask.py           # Question answering endpoint
â”‚   â”‚   â””â”€â”€ health.py        # Health check endpoints
â”‚   â”œâ”€â”€ services/            # Business logic
â”‚   â”‚   â”œâ”€â”€ rag_service.py   # RAG orchestration
â”‚   â”‚   â”œâ”€â”€ embedding_service.py  # Text embeddings
â”‚   â”‚   â””â”€â”€ llm_service.py   # LLM inference
â”‚   â”œâ”€â”€ models/              # Pydantic schemas
â”‚   â”‚   â””â”€â”€ schemas.py       # Request/Response models
â”‚   â”œâ”€â”€ core/                # Configuration
â”‚   â”‚   â””â”€â”€ config.py        # Settings management
â”‚   â”œâ”€â”€ utils/               # Utilities
â”‚   â”‚   â”œâ”€â”€ pdf_parser.py    # PDF text extraction
â”‚   â”‚   â””â”€â”€ text_chunker.py  # Text chunking
â”‚   â””â”€â”€ db/                  # Vector store
â”‚       â””â”€â”€ faiss_store.py   # FAISS index management
â”œâ”€â”€ data/                    # Stored documents & index
â”œâ”€â”€ requirements.txt
â””â”€â”€ .env.example
```

## ğŸš€ Quick Start

### 1. Install Dependencies

```bash
cd backend
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
pip install -r requirements.txt
```

### 2. Configure Environment

```bash
cp .env.example .env
# Edit .env with your settings
```

### 3. Run the Server

```bash
# From backend directory
python -m app.main

# Or with uvicorn directly
uvicorn app.main:app --reload --port 8000
```

### 4. Access API Documentation

- Swagger UI: http://localhost:8000/docs
- ReDoc: http://localhost:8000/redoc

## ğŸ“¡ API Endpoints

### Upload Document

```bash
POST /upload
Content-Type: multipart/form-data

curl -X POST "http://localhost:8000/upload" \
  -F "file=@document.pdf"
```

### Ask Question

```bash
POST /ask
Content-Type: application/json

curl -X POST "http://localhost:8000/ask" \
  -H "Content-Type: application/json" \
  -d '{"question": "What are fundamental rights in Nepal?", "top_k": 5}'
```

### Health Check

```bash
GET /health
GET /stats
```

## ğŸ”§ Configuration

Key settings in `.env`:

| Variable          | Description                 | Default                                  |
| ----------------- | --------------------------- | ---------------------------------------- |
| `EMBEDDING_MODEL` | HuggingFace embedding model | `sentence-transformers/all-MiniLM-L6-v2` |
| `LLM_MODEL`       | LLaMA model for generation  | `meta-llama/Llama-2-7b-chat-hf`          |
| `CHUNK_SIZE`      | Characters per chunk        | `500`                                    |
| `CHUNK_OVERLAP`   | Overlap between chunks      | `50`                                     |
| `TOP_K_RESULTS`   | Default search results      | `5`                                      |

## ğŸ§  How It Works

1. **Upload**: PDF â†’ Extract Text â†’ Chunk â†’ Embed â†’ Store in FAISS
2. **Query**: Question â†’ Embed â†’ Search FAISS â†’ Build Prompt â†’ LLM â†’ Answer

## ğŸ“ Notes

- For GPU acceleration, install `faiss-gpu` instead of `faiss-cpu`
- LLaMA models require HuggingFace authentication
- The system falls back to mock responses without GPU/LLM
